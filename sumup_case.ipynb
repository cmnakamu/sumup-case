{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install pandas google-cloud-bigquery google-cloud-storage"
      ],
      "metadata": {
        "id": "eYbwHowKF_Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Upload the service account JSON file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "svtDwX7ZGBOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Authenticate with the service account\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Replace 'your_service_account.json' with the filename of the uploaded JSON key\n",
        "service_account_info = json.load(open('sumup_case_sa.json'))\n",
        "project_id = service_account_info['project_id']\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "credentials = service_account.Credentials.from_service_account_info(service_account_info)\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "bq_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "storage_client = storage.Client(credentials=credentials, project=project_id)"
      ],
      "metadata": {
        "id": "kK2O0w_rGFMu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert Excel Files to CSV\n",
        "import pandas as pd\n",
        "\n",
        "excel_files = ['store.xlsx', 'device.xlsx', 'transaction.xlsx']\n",
        "csv_files = ['stores.csv', 'devices.csv', 'transactions.csv']\n",
        "\n",
        "for excel_file, csv_file in zip(excel_files, csv_files):\n",
        "    df = pd.read_excel(excel_file)\n",
        "    df.to_csv(csv_file, index=False)"
      ],
      "metadata": {
        "id": "fWPoRkEzGHf4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Upload CSV Files to Google Cloud Storage\n",
        "bucket_name = \"sumup_case\"\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "for file in csv_files:\n",
        "    blob = bucket.blob(file)\n",
        "    blob.upload_from_filename(file)\n"
      ],
      "metadata": {
        "id": "NDXT40-SGKLH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ-QjzeVDP_d",
        "outputId": "1cf7afeb-6166-407b-d4f7-07eabb52d690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 rows into SumUp_Case:stores\n",
            "Loaded 200 rows into SumUp_Case:devices\n",
            "Loaded 1500 rows into SumUp_Case:transactions\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Load CSV Data into BigQuery\n",
        "def load_data_from_gcs_to_bq(dataset_name, table_name, gcs_uri):\n",
        "    dataset_ref = bq_client.dataset(dataset_name)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        autodetect=True,\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    )\n",
        "\n",
        "    load_job = bq_client.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        table_ref,\n",
        "        job_config=job_config\n",
        "    )\n",
        "    load_job.result()  # Waits for the job to complete\n",
        "\n",
        "    print(f\"Loaded {load_job.output_rows} rows into {dataset_name}:{table_name}\")\n",
        "\n",
        "dataset_name = 'SumUp_Case'\n",
        "bucket_uri = f\"gs://{bucket_name}/\"\n",
        "\n",
        "load_data_from_gcs_to_bq(dataset_name, 'stores', f\"{bucket_uri}stores.csv\")\n",
        "load_data_from_gcs_to_bq(dataset_name, 'devices', f\"{bucket_uri}devices.csv\")\n",
        "load_data_from_gcs_to_bq(dataset_name, 'transactions', f\"{bucket_uri}transactions.csv\")\n"
      ]
    }
  ]
}