{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ELT pipeline\n",
        "6 steps to extract the data from the excel files into the bigquery datawarehouse where the SQL queries will be perform to answer the business questions."
      ],
      "metadata": {
        "id": "Ck4gPKACbTZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install pandas google-cloud-bigquery google-cloud-storage bcchapi"
      ],
      "metadata": {
        "id": "eYbwHowKF_Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Upload the service account JSON file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "svtDwX7ZGBOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In step 3 you access my GCP account through a service account to load the excel into a bigquery project:"
      ],
      "metadata": {
        "id": "JS_osltKYpLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Authenticate with the service account\n",
        "import os\n",
        "import json\n",
        "\n",
        "service_account_info = json.load(open('sumup_case_sa.json'))\n",
        "project_id = service_account_info['project_id']\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "credentials = service_account.Credentials.from_service_account_info(service_account_info)\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "bq_client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "storage_client = storage.Client(credentials=credentials, project=project_id)"
      ],
      "metadata": {
        "id": "kK2O0w_rGFMu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps 4 to 6 extract the data and load it into the datawarehouse.\n",
        "\n",
        "It is necessary to upload the 3 excel files to Google Colab (drag them to the section with the folder icon in the menu on the left). It could have been made more sophisticated (e.g. loading it from a shared folder in the cloud), but I think it was beyond the focus of the exercise."
      ],
      "metadata": {
        "id": "xO5oCHGVZ1Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert Excel Files to CSV\n",
        "import pandas as pd\n",
        "\n",
        "excel_files = ['store.xlsx', 'device.xlsx', 'transaction.xlsx']\n",
        "csv_files = ['stores.csv', 'devices.csv', 'transactions.csv']\n",
        "\n",
        "for excel_file, csv_file in zip(excel_files, csv_files):\n",
        "    df = pd.read_excel(excel_file)\n",
        "    df.to_csv(csv_file, index=False)"
      ],
      "metadata": {
        "id": "fWPoRkEzGHf4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Upload CSV Files to Google Cloud Storage\n",
        "bucket_name = \"sumup_case\"\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "for file in csv_files:\n",
        "    blob = bucket.blob(file)\n",
        "    blob.upload_from_filename(file)\n"
      ],
      "metadata": {
        "id": "NDXT40-SGKLH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ-QjzeVDP_d"
      },
      "outputs": [],
      "source": [
        "# Step 6: Load CSV Data into BigQuery\n",
        "def load_data_from_gcs_to_bq(dataset_name, table_name, gcs_uri):\n",
        "    dataset_ref = bq_client.dataset(dataset_name)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        autodetect=True,\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # If the tables already exist, it rewrites them instead of duplicating the data, so you can run the script several times without problem\n",
        "    )\n",
        "\n",
        "    load_job = bq_client.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        table_ref,\n",
        "        job_config=job_config\n",
        "    )\n",
        "    load_job.result()  # Waits for the job to complete\n",
        "\n",
        "    print(f\"Loaded {load_job.output_rows} rows into {dataset_name}:{table_name}\")\n",
        "\n",
        "dataset_name = 'SumUp_Case'\n",
        "bucket_uri = f\"gs://{bucket_name}/\"\n",
        "\n",
        "load_data_from_gcs_to_bq(dataset_name, 'stores', f\"{bucket_uri}stores.csv\")\n",
        "load_data_from_gcs_to_bq(dataset_name, 'devices', f\"{bucket_uri}devices.csv\")\n",
        "load_data_from_gcs_to_bq(dataset_name, 'transactions', f\"{bucket_uri}transactions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UF Value ingestion:\n",
        "The time window select for this series is YTD."
      ],
      "metadata": {
        "id": "LV7H-kuzEa7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Fetch Data from Banco Central de Chile API\n",
        "import bcchapi\n",
        "\n",
        "# datetime imports for building the YTD logic for the UF series:\n",
        "from datetime import datetime\n",
        "\n",
        "# Api credentials:\n",
        "siete = bcchapi.Siete(file=\"credenciales.txt\")\n",
        "\n",
        "# Get the current date\n",
        "today = datetime.today()\n",
        "\n",
        "# Set the first day of the year\n",
        "first_day_of_year = datetime(today.year, 1, 1).strftime('%Y-%m-%d')\n",
        "\n",
        "# Set today\n",
        "today_str = today.strftime('%Y-%m-%d')\n",
        "\n",
        "# Create the table using the UF series code. The output is a panda dataframe object:\n",
        "uf_df = siete.cuadro(\n",
        "    series=[\"F073.UFF.PRE.Z.D\"],\n",
        "    nombres=[\"uf\"],\n",
        "    desde=first_day_of_year,\n",
        "    hasta=today_str\n",
        ")\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "uf_csv_file = 'uf_data.csv'\n",
        "uf_df.to_csv(uf_csv_file, index=True)\n",
        "\n",
        "# Step 8: Upload the UF CSV to Google Cloud Storage\n",
        "uf_blob = bucket.blob(uf_csv_file)\n",
        "uf_blob.upload_from_filename(uf_csv_file)\n",
        "\n",
        "# Step 9: Load the UF CSV Data into BigQuery\n",
        "load_data_from_gcs_to_bq(dataset_name, 'uf_data', f\"{bucket_uri}{uf_csv_file}\")\n"
      ],
      "metadata": {
        "id": "bLJ2h0URNUzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}